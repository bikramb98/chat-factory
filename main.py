from dotenv import find_dotenv, load_dotenv
from huggingface_hub import InferenceClient
import os
from openai import OpenAI
from sql_handler import SQLQueryHandler
from rag_handler import RAGQueryHandler

def hf_client_call(client_type, sys_prompt, query):
    response = ""
    for message in client_type.chat_completion(
        messages=[{"role": "system", "content": f"{sys_prompt}"},
            {"role": "user", "content": f"{query}"}],
        max_tokens=500,
        stream=True):
        content = message.choices[0].delta.content
        response += content
    return response

def openai_call(model, sys_prompt, user_query):
    client = OpenAI(api_key=os.environ.get("OPENAI_API_TOKEN"))
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_query}
        ],
        temperature=0.7,
        max_tokens=500
    )
    return response.choices[0].message.content

def main():
    # Load environment variables
    load_dotenv(find_dotenv())
    
    # Initialize HuggingFace clients
    HUGGINGFACE_API_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
    qwen_client = InferenceClient(
        "Qwen/Qwen2.5-Coder-32B-Instruct",
        token=HUGGINGFACE_API_TOKEN,
    )
    
    # Load prompts
    with open('query_type_sys_prompt.txt', 'r') as f:
        query_type_sys_prompt = f.read()
    with open('system_prompt.txt', 'r') as f:
        sql_gen_sys_prompt = f.read()
    with open('response_sys_prompt.txt', 'r') as f:
        response_prompt = f.read()
    with open('machine_name_prompt.txt', 'r') as f:
        machine_name_sys_prompt = f.read()
    
    # Example user question
    user_question = "How to fix the coolant leakage in the laser cutter?"
    
    # Determine query type
    query_type = openai_call('gpt-4', query_type_sys_prompt, user_question)
    print(f'QUERY TYPE IS -------> {query_type}')
    
    if query_type == 'SQL':
        # Handle SQL query
        sql_handler = SQLQueryHandler()
        sql_query = hf_client_call(qwen_client, sql_gen_sys_prompt, user_question)
        
        if sql_query:
            result = sql_handler.execute_query(sql_query)
            if result:
                response = hf_client_call(
                    qwen_client,
                    f"You are an expert in converting a SQL query results into a sentence, with the knowledge of what the query is about. Only response back with the sentence, generated by using the SQL query response and the context that generated it. You also have the following information: {response_prompt}",
                    f"The question that the user asked is {user_question}. The column values are given in {sql_gen_sys_prompt}. The SQL query response is {result}. Add knowledge from your system prompt regarding the various columns in the table. Frame a sentence to communicate this to the user."
                )
                print("Response ----> ", response)
    
    else:
        # Handle RAG query
        rag_handler = RAGQueryHandler()
        machine_name = openai_call("gpt-4", machine_name_sys_prompt, user_question)
        print(f"machine_name is ------> {machine_name}")
        
        answer = rag_handler.generate_answer(user_question, machine_name)
        print(answer)

if __name__ == "__main__":
    main()