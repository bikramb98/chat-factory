from dotenv import find_dotenv, load_dotenv
from huggingface_hub import InferenceClient
import os
import streamlit as st
import re
import pymysql
from sqlalchemy import create_engine, text
import json
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai
import fitz
import faiss
import numpy as np
from openai import OpenAI

user_question = "When was pneumatic cylinders last updated in inventory?"

def hf_client_call(client_type, sys_prompt, query):

    response = ""

    for message in client_type.chat_completion(
        messages=[{"role": "system", "content": f"{sys_prompt}"},
            {"role": "user", "content": f"{query}"}],
        max_tokens=500,
        stream=True):

        content = message.choices[0].delta.content
        response += content
    
    return response


# Database connection details
with open("db_config.json", "r") as file:
    config = json.load(file)

# Extract details
DB_USER = config["DB_USER"]
DB_PASSWORD = config["DB_PASSWORD"]
DB_NAME = config["DB_NAME"]
PUBLIC_IP = config["PUBLIC_IP"]
PORT = config["PORT"]
# Create connection string
connection_url = f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{PUBLIC_IP}:{PORT}/{DB_NAME}"

# Create engine
engine = create_engine(connection_url)

#open and read system_prompt.txt file
sql_gen_sys_prompt_file = open('system_prompt.txt', 'r')
sql_gen_sys_prompt = sql_gen_sys_prompt_file.read()

response_prompt_file = open('response_sys_prompt.txt', 'r') 
response_prompt = response_prompt_file.read()

machine_name_sys_prompt_file = open('machine_name_prompt.txt', 'r')
machine_name_sys_prompt = machine_name_sys_prompt_file.read()

query_type_sys_prompt_file = open('query_type_sys_prompt.txt', 'r')
query_type_sys_prompt = query_type_sys_prompt_file.read()

# print(sys_prompt)

load_dotenv(find_dotenv())

HUGGINGFACE_API_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

llama_client = InferenceClient(
    "meta-llama/Meta-Llama-3-8B-Instruct",
    token=HUGGINGFACE_API_TOKEN,
)

qwen_client = InferenceClient(
    "Qwen/Qwen2.5-Coder-32B-Instruct",
    token=HUGGINGFACE_API_TOKEN,
)

openai_client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"), 
)

def openai_call(model, sys_prompt, user_query):

    response = openai_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_query}
            ],
            temperature=0.7,
            max_tokens=500
        )
    
    return response.choices[0].message.content

# query_type = hf_client_call(llama_client, query_type_sys_prompt, user_question)
query_type = openai_call('gpt-4', query_type_sys_prompt, user_question)


print(f'QUERY TYPE IS -------> {query_type}')

if query_type == 'SQL':

    sql_query = hf_client_call(qwen_client, sql_gen_sys_prompt, user_question)

    if sql_query != "":

        sql_query = text(f"""{sql_query}""")

        # Execute the query and fetch the result
        try:
            with engine.connect() as connection:
                result = connection.execute(sql_query)
                row = result.fetchall()  # Fetch the first row
                print(row)

        except Exception as e:
            print(f"Error: {e}")

        finally:
            engine.dispose()  # Close the database connection

        sql_query_response = hf_client_call(qwen_client, f"You are an expert in converting a SQL query results into a sentence, with the knowledge of what the query is about. Only response back with the sentence, generated by using the SQL query response and the context that generated it. You also have the following informaiton: {response_prompt}",
                                            f"The question that the user asked is {user_question}. The column values are given in {sql_gen_sys_prompt}. The SQL query response is {row}. Add knowledge from your system prompt regarding the various columns in the table. Frame a sentence to communicate this to the user.")

    ###############xxxxxxxxxxx##############

    print("Response ----> ", sql_query_response)

else:

    # query = 'Why is tool misalignment in the laser cutter?'

    # Step 1: Load PDF and Extract Text
    def extract_text_from_pdf(pdf_path):
        text = ""
        with fitz.open(pdf_path) as doc:
            for page in doc:
                text += page.get_text("text") + "\n"
        return text

    # Step 2: Chunk the text for better retrieval
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # Size of each chunk
        chunk_overlap=100  # Overlapping for better context retention
    )

    # Load PDF files and extract text
    documents = []
    document_mappings = {}  # Store which chunks belong to which machine
    metadata_store = []  # Stores metadata (machine type) corresponding to each embedding
    chunk_registry = {}
    chunk_counter = 0

    machine_names = {
        "cnc_guide.pdf": "CNC Lathe-1000",
        "hydraulic_press_guide.pdf": "Hydraulic Press-200",
        "laser_cutter_guide.pdf": "Laser Cutter-X5"
    }

    pdf_folder = "machine_manuals"  # Change this to the folder containing PDFs
    # for file in os.listdir(pdf_folder):
    #     if file.endswith(".pdf"):
    #         pdf_path = os.path.join(pdf_folder, file)
    #         extracted_text = extract_text_from_pdf(pdf_path)
    #         chunks = text_splitter.split_text(extracted_text)
    #         documents.extend(chunks)

    for file in os.listdir(pdf_folder):
        if file.endswith(".pdf"):
            machine_type = machine_names.get(file)  # Get machine type from filename
            pdf_path = os.path.join(pdf_folder, file)
            
            extracted_text = extract_text_from_pdf(pdf_path)
            text_chunks = extracted_text.split("\n\n")  # Basic chunking
            
            for chunk in text_chunks:
                documents.append(chunk)
                document_mappings[len(metadata_store)] = machine_type  # Store chunk index with machine type
                metadata_store.append(machine_type)

    # Step 3: Convert documents into embeddings
    embedding_model = OpenAIEmbeddings()
    embeddings = [embedding_model.embed_query(chunk) for chunk in documents]
    embeddings = np.array(embeddings, dtype="float32")

    # Step 3: Convert documents into embeddings and store them
    # embeddings = np.array([embedding_model.encode(doc) for doc in documents], dtype="float32")
    # index.add(embeddings)

    # Step 4: Store embeddings in FAISS index
    d = embeddings.shape[1]  # Vector dimension
    index = faiss.IndexFlatL2(d)
    index.add(embeddings)

    # Step 5: Query Processing & Retrieval
    def retrieve_relevant_docs(query, machine_type, k=3):
        query_embedding = embedding_model.embed_query(query)
        query_embedding = np.array([query_embedding], dtype="float32")
        distances, indices = index.search(query_embedding, k)
        filtered_results = []
        for i in indices[0]:
            if document_mappings[i] == machine_type:
                filtered_results.append(documents[i])
            if len(filtered_results) >= k:
                break  # Stop once we have enough filtered results
        
        return filtered_results

    def generate_answer_with_rag(query, machine_type):
        retrieved_docs = retrieve_relevant_docs(query, machine_type)
        context = "\n".join(retrieved_docs)

        print(f"context is {context}")

        # if not context.strip():
        #     return "I don't know the answer based on the available information."

        prompt = f"""Given this context about {machine_type}:
        {context}

        Question: {query}

        Provide a clear answer:"""

        try:
            response = openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert in troubleshooting manufacturing machines. You must strictly answer only based on the given context. If the answer is not found in the context, say: 'I don't know the answer based on the available information.' Do not infer anything beyond the context."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error in generation: {e}")
            return "Error generating response"

    # Example Query
    # test_query = 


    # Get machine query




    # def get_machine_name(test_query):

    #     machine_name = ""

    #     for message in llama_client.chat_completion(
    #         messages=[{"role": "system", "content": f"{machine_name_sys_prompt}"},
    #             {"role": "user", "content": f"{test_query}"}],
    #         max_tokens=500,
    #         stream=True):

    #         content = message.choices[0].delta.content
    #         machine_name += content

    #     return machine_name

    # machine_query = "Laser Cutter-X5"

    # machine_query = get_machine_name(test_query)
    # machine_query = hf_client_call(llama_client, machine_name_sys_prompt, test_query)

    machine_name = openai_call("gpt-4", machine_name_sys_prompt, user_question)

    print(f"machine_name is ------> {machine_name}")

    answer = generate_answer_with_rag(user_question, machine_name)
    print("\nGenerated Response:", answer)